{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d21a4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802e0cb",
   "metadata": {},
   "source": [
    "### 1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "> * Lowercase everything\n",
    "> * Normalize unicode characters\n",
    "> * Replace anything that is not a letter, number, whitespace or a single quote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54098d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'real words here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c222ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(s: str):\n",
    "    '''Takes in string and converts everything to lowercase\n",
    "    normalizes unicode characters and replaces everything that's\n",
    "    not a number letter or whitespace with a single quote.\n",
    "    '''\n",
    "    return  re.sub(r\"[^a-z0-9'\\s]\",\n",
    "                   '', unicodedata.normalize('NFKD', s.lower()\n",
    "                                  ).encode('ascii', 'ignore'\n",
    "                                          ).decode('utf-8', 'ignore'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6b04d",
   "metadata": {},
   "source": [
    "### 2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e84e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real', 'words', 'here']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(s: str):\n",
    "    '''Tokenizes all words within the string\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    return tokenizer.tokenize(s)\n",
    "\n",
    "tokenize(basic_clean(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cdc5d9",
   "metadata": {},
   "source": [
    "### 3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d175ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'real word here'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(s: str):\n",
    "    '''Accepts string and returns the string after they have been stemmed\n",
    "    '''\n",
    "    # Create stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "\n",
    "    return ' '.join([ps.stem(word) for word in s.split()])\n",
    "\n",
    "test = stem(' '.join(tokenize(basic_clean(test))))\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5904f6",
   "metadata": {},
   "source": [
    "### 4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949a3940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real', 'word', 'here']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(s: str):\n",
    "    '''Takes a string and returns a lemmatization of that string\n",
    "    '''\n",
    "    # Create the Word Net Lemmatizer\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    \n",
    "    return [wnl.lemmatize(word) for word in s.split()]\n",
    "    \n",
    "lemmatize(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b82ee",
   "metadata": {},
   "source": [
    "### 5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "### This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657561b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real', 'word']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(s: str, extra_words=list(), exclude_words=list()):\n",
    "    '''Takes a string and returns a string with the stopwords removed\n",
    "    also adds the desired extrawords and excluded words\n",
    "    '''\n",
    "    # Imports the english stopwords\n",
    "    stopword_list = stopwords.words('english')\n",
    "\n",
    "    # What words you'd like to include\n",
    "    stopword_list.extend(extra_words)\n",
    "    \n",
    "    # Iterate though each desired exclusionary words\n",
    "    for word in exclude_words:\n",
    "        # remove word from stoplist\n",
    "        stopword_list.remove(word)\n",
    "        \n",
    "    # Remove words from the string\n",
    "    words = s.split()\n",
    "    # Add all the words that are NOT in the stoplist \n",
    "    return [word for word in words if word not in stopword_list]\n",
    "    \n",
    "remove_stopwords(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c55214",
   "metadata": {},
   "source": [
    "### 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9558e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
